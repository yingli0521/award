---
title: "Paper Draft"
author: "Ying Li"
date: "April 18, 2018"
output:
  pdf_document: default
  html_document:
    df_print: paged
geometry: left = 2.54cm, right = 2.54cm, top = 2.54cm, bottom = 2.54cm
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T, tidy=T, tidy.opts=list(width.cutoff=80),
                      size="small",    
                      results='markup',
                      strip.white=TRUE,
                      fig.path='figs/fig',
                      cache=FALSE,
                      highlight=TRUE,
                      width.cutoff=80,
                      size='footnotesize',
                      out.width='.9\\textwidth',
                      message=FALSE,
                      comment=NA)
```
Thoughts about further moves:

combine random effects into the model, but may need to research how to use lmer for a binary response variable.

maybe something about senstivity test?


# Data Source

This original data (in a textual form) are obtained from the official website of [China Enterprise Confederation (CEC)](http://www.cec1979.org.cn/glcx/index.php). CEC is a national social organization whose mission is to facilitate and celebrate Chinese firm's reform and development, but in fact it is an informal government body who is responsible for regularly reporting to China's communist party how good the nation-wide state-owned enterprises (SOE) are operating and performing. 
In 1990, CEC innitiated an annual innovation award^[The first five were biannual. Since 2000 it became annual.], aiming to encourage creativity and stimulate vitality of China's SOEs. Every SOE^[Occasionally, non-SOEs also applied for the award, but they were always the minority of applicants.] is eligible to apply for this award by submitting a report of their innovation accomplishment. CEC then organizes a panel comprised of experts from governments and universities to evaluate these reports and select a number of award-winning companies every year. 

From 1990 to 2011, there were altogether 1637 application reports, which are all available on CEC's offcial website. Each application report, with an average of appropriately 8000 Chinese words, is a thorough description of a firm's innovation practices. These firms are from various industries, and the innovation areas cover various management functions such as HR management, R&D, corporate governance, finance, etc..

# Research Question and Hypothesis
I intend to investigate how stakeholders' evaluations of a firm can be influenced by the framing strategies that firms use to project their identity. Specifically, with the current data, my research question is how firms' category spanning behaviors across multiple innovation areas affect the probability of winning a government innovatin award. Based on the extant literature between category and evaluation, I hypothesize that:

 *Hypothesis: The category spanning behavior across innovation areas leads to a lower award-winning probability.*

# Variables
```{r} 
mydata <- read.csv("award.csv", header = T) 
# I understand that you may not have access to the data, I'll figure out a better idea of how to make it available.
# Question: Where do you store your data and share it with you coauthors? By your personal website? Is it a legitimate concern that I don't want to make my data publicly available before the paper is pulished?
row.names(mydata) <- mydata$appid

# a little data cleaning
mydata$year <- substr(mydata$appid, 3, 6) ## which year a SOE applied for the award
# The following three codes intend to make factor variables
mydata$area1 <- factor(mydata$area, 
                      levels=c(1:8),
                      labels=c("Northest","North","East","Central",
                               "South","Southwest","Northwest","Overseas"))
mydata$ownership1 <- factor(mydata$ownership, 
                      levels=c(1:6),
                      labels=c("Central","Ordinary","Collective", "Joint","Foreign", "Private"))
mydata$scale1 <- factor(mydata$scale,
                        levels = c(1:3),
                        labels = c("Large","Medium","Small"))
mydata$year <- as.numeric(mydata$year)

## Codebook for column 2 to column 51:
## 1. industry topics (col2:col26):
## topic1 Real Estate;
## topic2 Electricity;
## topic3 Agriculture...
## 2. innovation area topics (col27:col51):
## topic5 Environment Conservation;
## topic6 Factory and workshop management;
## topic8 Retailing...
## 3. award: 1 "wining an award"; 0 "losing an award"
```

```{r}
# To determine the industry of the firm
indtpc <- mydata[,2:26]
indvar <- apply(indtpc, 1, sd)
indmax <- apply(indtpc, 1, max)
industry <- NULL
for(j in 1:nrow(indtpc)){
    for(i in 1:ncol(indtpc)){
      if(indtpc[j,i]==indmax[j]){
        industry[j] <- colnames(indtpc)[i]
      }
    }
} # The industry varaible is represented by different topics.

# The extent of how severe a firm's category spanning behavior is. The larger the `innvar`, the less severe the category spanning behavior.
inntpc <- mydata[,27:51]
max1 <- NULL; max2 <- NULL;max3 <- NULL
for(j in 1:nrow(indtpc)){
  max1[j] <- inntpc[j,order(inntpc[j,],decreasing=TRUE)[1]]
  max2[j] <- inntpc[j,order(inntpc[j,],decreasing=TRUE)[2]]
  max3[j] <- inntpc[j,order(inntpc[j,],decreasing=TRUE)[3]]
}
max1 <- as.numeric(max1);max2 <- as.numeric(max2);max3 <- as.numeric(max3)
max <- matrix(c(max1,max2,max3),byrow=F,ncol=3)
innvar <- apply(max,1,sd)

## make a smaller dataset
newdata <- data.frame(mydata$award, mydata[,56:59],industry, indvar, innvar)
colnames(newdata) <- c("award","year","area","ownership","scale","industry","indvar", "innvar")
newdata$year <- as.factor(newdata$year)
# str(newdata)
```
## Dependent Variable
```{r,echo=F, results = 'hide'}
meany <- mean(mydata$award) ## Question: do the success and failure events have to be nearly equal for a logit model? Create training sets and test sets?
meany1 <- sprintf("%.1f%%", meany * 100)
```
*Award winning*. The outcome variable is whether a firm won a award, where 1 indicates winning and 0 indicates losing. `r meany1` of the applicants won an award in this dataset.

## Independent variable
*Innovation variability* meansures the extent of a firm's category spanning behavior across innovation areas. Before constructing this variable, all the textual data have been preliminarily analyzed through a textual analysis technique called topic modeling. As a result, all the application reports are divided into 50 topics, 25 of which are about industries and the other 25 are about innovation areas^[While how the topic modelling process was conducted is beyond the scope of this paper, I would like to emphasize that the interpretation of the topic themes still depends on human rather than machine. I reached the conclusion of whether a topic belongs to the industry category or the innovation category by consulting an experienced panel expert for CEC]. And each application report is assigned a coverage proportaion of every topic, producing 1637*50 topic coverage proportion numbers. In the following analysis, `innvar` refers to innovation variability. I measure this variable by calculating the standard deviation of the coverage proportions in the three largest innovation topics in an application report. The higher the `innvar`, the less severe a firm's category spanning behavior is. 

## Control Variables

I included five control variables that could potentially influence the award winning probability. *Area*. Chinese government tends to balance the awards-winning opportunites for firms in different geographic areas. *Scale*. Whether an applicant firm is large, medium, or small-size affects its ability to innovate and thus affect its winning probability. *Ownership*. Whether an applicant firm is a Central SOE, ordinary SOE, or other forms. *Industry variation*. Chinese SOEs are often big companies that operate across multiple industries. Government may treat firms differently if they express their multi-industry identities in the application reports. I measure this variable by calcuting the standard deviation of topic coverage proportions across all the industry topics. *Industry*. Different industries have different evaluation logics, so it would be better to control within industries. I measure the industry variable by refering to the industry topic that has the largest coverage proportion.

# Data Description
```{r, results='asis'}
library(stargazer)
data <- cbind(mydata[,52:56],newdata[,7:8])
cor <- cor(data[,1:7],method="pearson")
stargazer(data,header = F, title = "Description")
stargazer(cor, header = F, title = "Correlation")
# Question: I'm still not very sure about how to present a description and correlations table when most of my variables are categorical. Need to work on that.
```

# Model Specification
I'm interested in predicting the probability of award winning, so the estimation method is maximum likelihood. The outcome variable is binary, so I use a logit transformation implemented via a generalized linear model to ensure that predictions of the dependent variable are strictly bounded between 0 and 1.
```{r}
# My thinking process
# (1) I don't have a sample (or a population). The data are all the firms that applied for the award from 1990 to 2011. So I need maximum likelihood method to estimate and test.
# (2) No firm actually appeared in multiple years or applied for multiple years in a given year, so cluster design is not necessary. Question: Is this argument right?
model <- glm(award~ area + ownership + scale + industry + indvar + innvar, data = newdata, family= binomial(link=logit))
summary(model)
# Question: I need to make the table look shorter...How to combine the different levels of one factor variable in just one line?
```

```{r}
# Question: How to evaluate this logit model? Is the following analysis necessary?
# Question: What should I do after I get the regression table...

# 1. AIC (Akaike Information Criteria) – The analogous metric of adjusted R² in logistic regression is AIC. AIC is the measure of fit which penalizes model for the number of model coefficients. Therefore, we always prefer model with minimum AIC value.

# 2. Null Deviance and Residual Deviance – Null Deviance indicates the response predicted by a model with nothing but an intercept. Lower the value, better the model. Residual deviance indicates the response predicted by a model on adding independent variables. Lower the value, better the model.

# 3. Confusion Matrix: It is nothing but a tabular representation of Actual vs Predicted values. This helps us to find the accuracy of the model and avoid overfitting.
predict <- predict(model, type = 'response')  ## Predict the probability
accuracy <- data.frame(table(newdata$award, predict > 0.5))
accrate <- (accuracy[1,3]+accuracy[4,3])/(accuracy[1,3]+accuracy[2,3]+accuracy[3,3]+accuracy[4,3])
## the accurate rate is
accrate

# 4. ROC Curve: Receiver Operating Characteristic(ROC) summarizes the model’s performance by evaluating the trade offs between true positive rate (sensitivity) and false positive rate(1- specificity). For plotting ROC, it is advisable to assume p > 0.5 since we are more concerned about success rate. ROC summarizes the predictive power for all possible values of p > 0.5.  The area under curve (AUC), referred to as index of accuracy(A) or concordance index, is a perfect performance metric for ROC curve. Higher the area under curve, better the prediction power of the model. Below is a sample ROC curve. The ROC of a perfect predictive model has TP equals 1 and FP equals 0. This curve will touch the top left corner of the graph.
library(ROCR)
ROCRpred <- prediction(predict, newdata$award)
ROCRperf <- performance(ROCRpred, 'tpr','fpr')
plot(ROCRperf, text.adj = c(-0.2,1.7))
```


# Results
`innvar` increase by 1, the log odds of winning an award increases by 1.71. This change is significant at the 0.05 level.

